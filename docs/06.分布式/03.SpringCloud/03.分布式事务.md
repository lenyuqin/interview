---
title: 分布式事务
date: 2021-04-12 21:54:59
permalink: /pages/8b7d96/
categories:
  - 分布式
  - SpringCloud
tags:
  - 
---
## 分布式事务

 Redis 的事务不能保证所有操作要么都执行要么都不执行，为什么它也叫事务啊？

Redis 怎么说的。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme8n9zHpx2IlrpyYSEWWaHj5wqkUTTo4UnIglLb6pJHtYM8ibxuTlNznrw/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这句话就是告诉大家事务中的某个命令失败了，之后的命令还是会被处理，Redis  不会停止命令，意味着也不会回滚。

来看看 Redis 怎么解释的。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme8rkDlzA7xWIdlJJV9LsMdw7tXpetIjJ4Ap6su4PTF6eQmWJRzEVMcKA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

Redis 官网解释了为什么不支持回滚，他们说首先如果命令出错那都是语法使用错误，**是你们自己编程出错**，而且这种情况应该在开发的时候就被检测出来，不应在生产环境中出现。

然后 Redis 就是为了快！不需要提供回滚。

**回滚并不能使你免于编程错误**。而且一般这种错也不可能进入到生产环境，所以选择更加简单、快速的方法，我们不支持回滚。

就下来就开始分布式事务。



- 0. 前言

- 1. 单数据源事务 & 多数据源事务

- 2. 常见分布式事务解决方案

- - 2.1. 分布式事务模型
  - 2.2. 二将军问题和幂等性
  - 2.3. 两阶段提交（2PC） & 三阶段提交（3PC）方案
  - 2.4. TCC 方案
  - 2.5. 事务状态表方案
  - 2.6. 基于消息中间件的最终一致性事务方案

- 3. Seata in AT mode 的实现

- - 3.1. Seata in AT mode 工作流程概述
  - 3.2. Seata in AT mode 工作流程详述

## 2PC

2PC（Two-phase commit protocol），中文叫二阶段提交。**二阶段提交是一种强一致性设计**，2PC 引入一个事务协调者的角色来协调管理各参与者（也可称之为各本地资源）的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。

两个阶段的具体流程。

**准备阶段**协调者会给各参与者发送准备命令，你可以把准备命令理解成除了提交事务之外啥事都做完了。

同步等待所有资源的响应之后就进入第二阶段即提交阶段（注意提交阶段不一定是提交事务，也可能是回滚事务）。

假如在第一阶段所有参与者都返回准备成功，那么协调者则向所有参与者发送提交事务命令，然后等待所有事务都提交成功之后，返回事务执行成功。

让我们来看一下流程图。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme8FLuMib3gXy1SPIYeaaxFtoib1JWtVo7iaba3FQTs88FuZ4poDARXmg2qA/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

假如在第一阶段有一个参与者返回失败，那么协调者就会向所有参与者发送回滚事务的请求，即分布式事务执行失败。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme8scMRr5aRqR1uiaHN0bKNAcbcmqL0Sf2G5TNtuEDcYtQQqiczXq58HjwQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

那可能就有人问了，那第二阶段提交失败的话呢？

这里有两种情况。

第一种是**第二阶段执行的是回滚事务操作**，那么答案是不断重试，直到所有参与者都回滚了，不然那些在第一阶段准备成功的参与者会一直阻塞着。

第二种是**第二阶段执行的是提交事务操作**，那么答案也是不断重试，因为有可能一些参与者的事务已经提交成功了，这个时候只有一条路，就是头铁往前冲，不断的重试，直到提交成功，到最后真的不行只能人工介入处理。

大体上二阶段提交的流程就是这样，**我们再来看看细节**。

首先 2PC 是一个**同步阻塞协议**，像第一阶段协调者会等待所有参与者响应才会进行下一步操作，当然第一阶段的**协调者有超时机制**，假设因为网络原因没有收到某参与者的响应或某参与者挂了，那么超时后就会判断事务失败，向所有参与者发送回滚命令。

在第二阶段协调者的没法超时，因为按照我们上面分析只能不断重试！

### 协调者故障分析

**协调者是一个单点，存在单点故障问题**。

假设协调者在**发送准备命令之前**挂了，还行等于事务还没开始。

假设协调者在**发送准备命令之后**挂了，这就不太行了，有些参与者等于都执行了处于事务资源锁定的状态。不仅事务执行不下去，还会因为锁定了一些公共资源而阻塞系统其它操作。

假设协调者在**发送回滚事务命令之前**挂了，那么事务也是执行不下去，且在第一阶段那些准备成功参与者都阻塞着。

假设协调者在**发送回滚事务命令之后**挂了，这个还行，至少命令发出去了，很大的概率都会回滚成功，资源都会释放。但是如果出现网络分区问题，某些参与者将因为收不到命令而阻塞着。

假设协调者在**发送提交事务命令之前**挂了，这个不行，傻了！这下是所有资源都阻塞着。

假设协调者在**发送提交事务命令之后**挂了，这个还行，也是至少命令发出去了，很大概率都会提交成功，然后释放资源，但是如果出现网络分区问题某些参与者将因为收不到命令而阻塞着。

### 协调者故障，通过选举得到新协调者

因为协调者单点问题，因此我们可以通过选举等操作选出一个新协调者来顶替。

如果处于第一阶段，其实影响不大都回滚好了，在第一阶段事务肯定还没提交。

如果处于第二阶段，假设参与者都没挂，此时新协调者可以向所有参与者确认它们自身情况来推断下一步的操作。

假设有个别参与者挂了！这就有点僵硬了，比如协调者发送了回滚命令，此时第一个参与者收到了并执行，然后协调者和第一个参与者都挂了。

此时其他参与者都没收到请求，然后新协调者来了，它询问其他参与者都说OK，但它不知道挂了的那个参与者到底O不OK，所以它傻了。

问题其实就出在**每个参与者自身的状态只有自己和协调者知道**，因此新协调者无法通过在场的参与者的状态推断出挂了的参与者是什么情况。

虽然协议上没说，不过在实现的时候我们可以灵活的让协调者将自己发过的请求在哪个地方记一下，也就是日志记录，这样新协调者来的时候不就知道此时该不该发了？

但是就算协调者知道自己该发提交请求，那么在参与者也一起挂了的情况下没用，因为你不知道参与者在挂之前有没有提交事务。

如果参与者在挂之前事务提交成功，新协调者确定存活着的参与者都没问题，那肯定得向其他参与者发送提交事务命令才能保证数据一致。

如果参与者在挂之前事务还未提交成功，参与者恢复了之后数据是回滚的，此时协调者必须是向其他参与者发送回滚事务命令才能保持事务的一致。

所以说极端情况下还是无法避免数据不一致问题。

这个代码就是实现了 2PC，但是相比于2PC增加了写日志的动作、参与者之间还会互相通知、参与者也实现了超时。这里要注意，一般所说的2PC，不含上述功能，这都是实现的时候添加的。

2PC 是一种**尽量保证强一致性**的分布式事务，因此它是**同步阻塞**的，而同步阻塞就导致长久的资源锁定问题，**总体而言效率低**，并且存在**单点故障**问题，在极端条件下存在**数据不一致**的风险。

当然具体的实现可以变形，而且 2PC 也有变种，例如 Tree 2PC、Dynamic 2PC。

还有一点不知道你们看出来没，2PC 适用于**数据库层面的分布式事务场景**，而我们业务需求有时候不仅仅关乎数据库，也有可能是上传一张图片或者发送一条短信。

而且像 Java 中的 JTA 只能解决一个应用下多数据库的分布式事务问题，跨服务了就不能用了。

简单说下 Java 中 JTA，它是基于XA规范实现的事务接口，这里的 XA 你可以简单理解为基于数据库的 XA 规范来实现的 2PC。（至于XA规范到底是啥，篇幅有限，下次有机会再说）

接下来我们再来看看 3PC。

## 3PC

3PC 的出现是为了解决 2PC 的一些问题，相比于 2PC 它在**参与者中也引入了超时机制**，并且**新增了一个阶段**使得参与者可以利用这一个阶段统一各自的状态。

让我们来详细看一下。

3PC 包含了三个阶段，分别是**准备阶段、预提交阶段和提交阶段**，对应的英文就是：`CanCommit、PreCommit 和 DoCommit`。

看起来是**把 2PC 的提交阶段变成了预提交阶段和提交阶段**，但是 3PC 的准备阶段协调者只是询问参与者的自身状况，比如你现在还好吗？负载重不重？这类的。

而预提交阶段就是和 2PC 的准备阶段一样，除了事务的提交该做的都做了。

提交阶段和 2PC 的一样，让我们来看一下图。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme86EFEmv5s5sricicFs5t4J3XT8AVTWvhzic9wMYCkzwGVNBX2zJiaoGLt7w/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

不管哪一个阶段有参与者返回失败都会宣布事务失败，这和 2PC 是一样的（当然到最后的提交阶段和 2PC 一样只要是提交请求就只能不断重试）。

我们先来看一下 3PC 的阶段变更有什么影响。

首先**准备阶段的变更成不会直接执行事务**，而是会先去询问此时的参与者是否有条件接这个事务，因此**不会一来就干活直接锁资源**，使得在某些资源不可用的情况下所有参与者都阻塞着。

而**预提交阶段的引入起到了一个统一状态的作用**，它像一道栅栏，表明在预提交阶段前所有参与者其实还未都回应，在预处理阶段表明所有参与者都已经回应了。

假如你是一位参与者，你知道自己进入了预提交状态那你就可以推断出来其他参与者也都进入了预提交状态。

但是多引入一个阶段也多一个交互，因此**性能会差一些**，而且**绝大部分的情况下资源应该都是可用的**，这样等于每次明知可用执行还得询问一次。

我们再来看下参与者超时能带来什么样的影响。

我们知道 2PC 是同步阻塞的，上面我们已经分析了协调者挂在了提交请求还未发出去的时候是最伤的，所有参与者都已经锁定资源并且阻塞等待着。

那么引入了超时机制，参与者就不会傻等了，**如果是等待提交命令超时，那么参与者就会提交事务了**，因为都到了这一阶段了大概率是提交的，**如果是等待预提交命令超时，那该干啥就干啥了，反正本来啥也没干**。

然而超时机制也会带来数据不一致的问题，比如在等待提交命令时候超时了，参与者默认执行的是提交事务操作，但是**有可能执行的是回滚操作，这样一来数据就不一致了**。

当然 3PC 协调者超时还是在的，具体不分析了和 2PC 是一样的。

从维基百科上看，3PC 的引入是为了解决提交阶段 2PC 协调者和某参与者都挂了之后新选举的协调者不知道当前应该提交还是回滚的问题。

新协调者来的时候发现有一个参与者处于预提交或者提交阶段，那么表明已经经过了所有参与者的确认了，所以此时执行的就是提交命令。

所以说 3PC 就是通过引入预提交阶段来使得参与者之间的状态得到统一，也就是留了一个阶段让大家同步一下。

但是这也只能让协调者知道该怎么做，但不能保证这样做一定对，这其实和上面 2PC 分析一致，因为挂了的参与者到底有没有执行事务无法断定。

所以说 3PC 通过预提交阶段可以减少故障恢复时候的复杂性，但是不能保证数据一致，除非挂了的那个参与者恢复。

让我们总结一下， 3PC 相对于 2PC 做了一定的改进：引入了参与者超时机制，并且增加了预提交阶段使得故障恢复之后协调者的决策复杂度降低，但整体的交互过程更长了，性能有所下降，并且还是会存在数据不一致问题。

所以 2PC 和 3PC 都不能保证数据100%一致，因此一般都需要有定时扫描补偿机制。



## TCC

**2PC 和 3PC 都是数据库层面的，而 TCC 是业务层面的分布式事务**，就像我前面说的分布式事务不仅仅包括数据库的操作，还包括发送短信等，这时候 TCC 就派上用场了！

TCC 指的是`Try - Confirm - Cancel`。

- Try 指的是预留，即资源的预留和锁定，**注意是预留**。
- Confirm 指的是确认操作，这一步其实就是真正的执行了。
- Cancel 指的是撤销操作，可以理解为把预留阶段的动作撤销了。

其实从思想上看和 2PC 差不多，都是先试探性的执行，如果都可以那就真正的执行，如果不行就回滚。

比如说一个事务要执行A、B、C三个操作，那么先对三个操作执行预留动作。如果都预留成功了那么就执行确认操作，如果有一个预留失败那就都执行撤销动作。

我们来看下流程，TCC模型还有个事务管理者的角色，用来记录TCC全局事务状态并提交或者回滚事务。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme8g6iaCMWDl6yFUgf5eBha1TlG2k2MBUkYtO9uuYuILWRicbmkEMvNBMgQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到流程还是很简单的，难点在于业务上的定义，对于每一个操作你都需要定义三个动作分别对应`Try - Confirm - Cancel`。

因此 **TCC 对业务的侵入较大和业务紧耦合**，需要根据特定的场景和业务逻辑来设计相应的操作。

还有一点要注意，撤销和确认操作的执行可能需要重试，因此还需要保证**操作的幂等**。

相对于 2PC、3PC ，TCC 适用的范围更大，但是开发量也更大，毕竟都在业务上实现，而且有时候你会发现这三个方法还真不好写。不过也因为是在业务上实现的，所以**TCC可以跨数据库、跨不同的业务系统来实现事务**。

## 本地消息表

本地消息表其实就是利用了 **各系统本地的事务**来实现分布式事务。

本地消息表顾名思义就是会有一张存放本地消息的表，一般都是放在数据库中，然后在执行业务的时候 **将业务的执行和将消息放入消息表中的操作放在同一个事务中**，这样就能保证消息放入本地表中业务肯定是执行成功的。

然后再去调用下一个操作，如果下一个操作调用成功了好说，消息表的消息状态可以直接改成已成功。

如果调用失败也没事，会有 **后台任务定时去读取本地消息表**，筛选出还未成功的消息再调用对应的服务，服务更新成功了再变更消息的状态。

这时候有可能消息对应的操作不成功，因此也需要重试，重试就得保证对应服务的方法是幂等的，而且一般重试会有最大次数，超过最大次数可以记录下报警让人工处理。

可以看到本地消息表其实实现的是**最终一致性**，容忍了数据暂时不一致的情况。

## 消息事务

RocketMQ 就很好的支持了消息事务，让我们来看一下如何通过消息实现事务。

第一步先给 Broker 发送事务消息即半消息，**半消息不是说一半消息，而是这个消息对消费者来说不可见**，然后**发送成功后发送方再执行本地事务**。

再根据**本地事务的结果向 Broker 发送 Commit 或者 RollBack 命令**。

并且 RocketMQ 的发送方会提供一个**反查事务状态接口**，如果一段时间内半消息没有收到任何操作请求，那么 Broker 会通过反查接口得知发送方事务是否执行成功，然后执行 Commit 或者 RollBack 命令。

如果是 Commit 那么订阅方就能收到这条消息，然后再做对应的操作，做完了之后再消费这条消息即可。

如果是 RollBack 那么订阅方收不到这条消息，等于事务就没执行过。

可以看到通过 RocketMQ 还是比较容易实现的，RocketMQ 提供了事务消息的功能，我们只需要定义好事务反查接口即可。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/uChmeeX1Fpx2WyATKXiadnNrvpBPNyme85yCYdK8oUfjehf9OA3A3icLzX1TwiatNU6PkdqmQp0Y4pzY3HxHFJyvQ/640?wx_fmt=jpeg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到消息事务实现的也是最终一致性。

## 最大努力通知

**最大努力通知其实只是表明了一种柔性事务的思想**：我已经尽力我最大的努力想达成事务的最终一致了。

适用于对时间不敏感的业务，例如短信通知。

# 总结

可以看出 2PC 和 3PC 是一种强一致性事务，不过还是有数据不一致，阻塞等风险，而且只能用在数据库层面。

而 TCC 是一种补偿性事务思想，适用的范围更广，在业务层面实现，因此对业务的侵入性较大，每一个操作都需要实现对应的三个方法。

本地消息、事务消息和最大努力通知其实都是最终一致性事务，因此适用于一些对时间不敏感的业务。



## 0. 前言

从 CPU 到内存、到磁盘、到操作系统、到网络，计算机系统处处存在不可靠因素。工程师和科学家努力使用各种软硬件方法对抗这种不可靠因素，保证数据和指令被正确地处理。在网络领域有 TCP 可靠传输协议、在存储领域有 Raid5 和 Raid6 算法、在数据库领域有基于 ARIES 算法理论实现的事务机制……

这篇文章先介绍单机数据库事务的 ACID 特性，然后指出分布式场景下操作多数据源面临的困境，引出分布式系统中常用的分布式事务解决方案，这些解决方案可以保证业务代码在操作多个数据源的时候，能够像操作单个数据源一样，具备 ACID 特性。文章在最后给出业界较为成熟的分布式事务框架——Seata 的 AT 模式全局事务的实现。

## 1. 单数据源事务 & 多数据源事务

笔者在这里不再对这四种语义进行解释，了解单数据源事务及其 ACID 特性是读者阅读这篇文章的前提。单个数据库实现自身的事务特性是一个复杂又微妙的过程，例如 MySQL 的 InnoDB 引擎通过 Undo Log + Redo Log + ARIES 算法来实现。这是一个很宏大的话题，不在本文的描述范围，读者有兴趣的话可自行研究。

单数据源事务也可以叫做单机事务，或者本地事务。

在分布式场景下，一个系统由多个子系统构成，每个子系统有独立的数据源。多个子系统之间通过互相调用来组合出更复杂的业务。在时下流行的微服务系统架构中，每一个子系统被称作一个微服务，同样每个微服务都维护自己的数据库，以保持独立性。

例如，一个电商系统可能由购物微服务、库存微服务、订单微服务等组成。购物微服务通过调用库存微服务和订单微服务来整合出购物业务。用户请求购物微服务商完成下单时，购物微服务一方面调用库存微服务扣减相应商品的库存数量，另一方面调用订单微服务插入订单记录（为了后文描述分布式事务解决方案的方便，这里给出的是一个最简单的电商系统微服务划分和最简单的购物业务流程，后续的支付、物流等业务不在考虑范围内）。电商系统模型如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibT9YktR9L88D4SARkeCfnKWANHoVfeDBdEbHjoWtVGmemxT8iaMxKsmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在用户购物的业务场景中，shopping-service 的业务涉及两个数据库：库存库（repo_db）和订单库（repo_db），也就是 g 购物业务是调用多数据源来组合而成的。作为一个面向消费者的系统，电商系统要保证购物业务的高度可靠性，这里的可靠性同样有 ACID 四种语义。

但是一个数据库的本地事务机制仅仅对落到自己身上的查询操作（这里的查询是广义的，包括增删改查等）起作用，无法干涉对其他数据库的查询操作。所以，数据库自身提供的本地事务机制无法确保业务对多数据源全局操作的可靠性。

基于此，针对多数据源操作提出的分布式事务机制就出现了。

分布式事务也可以叫做全局事务。

## 2. 常见分布式事务解决方案

### 2.1. 分布式事务模型

描述分布式事务，常常会使用以下几个名词：

\- 事务参与者：例如每个数据库就是一个事务参与者

\- 事务协调者：访问多个数据源的服务程序，例如 shopping-service 就是事务协调者

\- 资源管理器（Resource Manager, RM）：通常与事务参与者同义

\- 事务管理器（Transaction Manager, TM）：通常与事务协调者同义

在分布式事务模型中，一个 TM 管理多个 RM，即一个服务程序访问多个数据源；TM 是一个全局事务管理器，协调多方本地事务的进度，使其共同提交或回滚，最终达成一种全局的 ACID 特性。

### 2.2. 二将军问题和幂等性

二将军问题是网络领域的一个经典问题，用于表达计算机网络中互联协议设计的微妙性和复杂性。这里给出一个二将军问题的简化版本：

一支白军被围困在一个山谷中，山谷的左右两侧是蓝军。困在山谷中的白军人数多于山谷两侧的任意一支蓝军，而少于两支蓝军的之和。若一支蓝军对白军单独发起进攻，则必败无疑；但若两支蓝军同时发起进攻，则可取胜。两只蓝军的总指挥位于山谷左侧，他希望两支蓝军同时发起进攻，这样就要把命令传到山谷右侧的蓝军，以告知发起进攻的具体时间。假设他们只能派遣士兵穿越白军所在的山谷（唯一的通信信道）来传递消息，那么在穿越山谷时，士兵有可能被俘虏。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibibXqFFfoX1zfgXtKHNPq6wiciaGliaiaX1ErWyeU9JQib6cSJJaiaKJldx36g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

只有当送信士兵成功往返后，总指挥才能确认这场战争的胜利（上方图）。现在问题来了，派遣出去送信的士兵没有回来，则左侧蓝军中的总指挥能不能决定按命令中约定的时间发起进攻？

答案是不确定，派遣出去送信的士兵没有回来，他可能遇到两种状况：

1）命令还没送达就被俘虏了（中间图），这时候右侧蓝军根本不知道要何时进攻；

2）命令送达，但返回途中被俘虏了（下方图），这时候右侧蓝军知道要何时进攻，但左侧蓝军不知道右侧蓝军是否知晓进攻时间。

类似的问题在计算机网络中普遍存在，例如发送者给接受者发送一个 HTTP 请求，或者 MySQL 客户端向 MySQL 服务器发送一条插入语句，然后超时了没有得到响应。请问服务器是写入成功了还是失败了？答案是不确定，有以下几种情况：

1）可能请求由于网络故障根本没有送到服务器，因此写入失败；

2）可能服务器收到了，也写入成功了，但是向客户端发送响应前服务器宕机了；

3）可能服务器收到了，也写入成功了，也向客户端发送了响应，但是由于网络故障未送到客户端。

无论哪种场景，在客户端看来都是一样的结果：它发出的请求没有得到响应。为了确保服务端成功写入数据，客户端只能重发请求，直至接收到服务端的响应。

类似的问题问题被称为网络二将军问题。

网络二将军问题的存在使得消息的发送者往往要重复发送消息，直到收到接收者的确认才认为发送成功，但这往往又会导致消息的重复发送。例如电商系统中订单模块调用支付模块扣款的时候，如果网络故障导致二将军问题出现，扣款请求重复发送，产生的重复扣款结果显然是不能被接受的。因此要保证一次事务中的扣款请求无论被发送多少次，接收方有且只执行一次扣款动作，这种保证机制叫做接收方的幂等性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibFNwHQyiaHPFJMKmXKesic8w5Gp6dZ3PVyDwKw2wYrFqsuyb4ResIxLaw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.3. 两阶段提交（2PC） & 三阶段提交（3PC）方案

2PC 是一种实现分布式事务的简单模型，这两个阶段是：

1）准备阶段：事务协调者向各个事务参与者发起询问请求：“我要执行全局事务了，这个事务涉及到的资源分布在你们这些数据源中，分别是……，你们准备好各自的资源（即各自执行本地事务到待提交阶段）”。各个参与者协调者回复 yes（表示已准备好，允许提交全局事务）或 no（表示本参与者无法拿到全局事务所需的本地资源，因为它被其他本地事务锁住了）或超时。

2）提交阶段：如果各个参与者回复的都是 yes，则协调者向所有参与者发起事务提交操作，然后所有参与者收到后各自执行本地事务提交操作并向协调者发送 ACK；如果任何一个参与者回复 no 或者超时，则协调者向所有参与者发起事务回滚操作，然后所有参与者收到后各自执行本地事务回滚操作并向协调者发送 ACK。

2PC 的流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibCs0Cv3zqCdAIjnU0eKxf9nvcjhsKg54MIs0LOKmQVR52yJmFPloVHQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从上图可以看出，要实现 2PC，所有的参与者都要实现三个接口：

- Prepare()：TM 调用该接口询问各个本地事务是否就绪
- Commit()：TM 调用该接口要求各个本地事务提交
- Rollback()：TM 调用该接口要求各个本地事务回滚

可以将这三个接口简单地（但不严谨地）理解成 XA 协议。XA 协议是 X/Open 提出的分布式事务处理标准。MySQL、Oracle、DB2 这些主流数据库都实现了 XA 协议，因此都能被用于实现 2PC 事务模型。

2PC 简明易懂，但存在如下的问题：

1）性能差，在准备阶段，要等待所有的参与者返回，才能进入阶段二，在这期间，各个参与者上面的相关资源被排他地锁住，参与者上面意图使用这些资源的本地事务只能等待。因为存在这种同步阻塞问题，所以影响了各个参与者的本地事务并发度；

2）准备阶段完成后，如果协调者宕机，所有的参与者都收不到提交或回滚指令，导致所有参与者“不知所措”；

3）在提交阶段，协调者向所有的参与者发送了提交指令，如果一个参与者未返回 ACK，那么协调者不知道这个参与者内部发生了什么（由于网络二将军问题的存在，这个参与者可能根本没收到提交指令，一直处于等待接收提交指令的状态；也可能收到了，并成功执行了本地提交，但返回的 ACK 由于网络故障未送到协调者上），也就无法决定下一步是否进行全体参与者的回滚。

2PC 之后又出现了 3PC，把两阶段过程变成了三阶段过程，分别是：询问阶段、准备阶段、提交或回滚阶段，这里不再详述。3PC 利用超时机制解决了 2PC 的同步阻塞问题，避免资源被永久锁定，进一步加强了整个事务过程的可靠性。但是 3PC 同样无法应对类似的宕机问题，只不过出现多数据源中数据不一致问题的概率更小。

2PC 除了性能和可靠性上存在问题，它的适用场景也很局限，它要求参与者实现了 XA 协议，例如使用实现了 XA 协议的数据库作为参与者可以完成 2PC 过程。但是在多个系统服务利用 api 接口相互调用的时候，就不遵守 XA 协议了，这时候 2PC 就不适用了。所以 2PC 在分布式应用场景中很少使用。

所以前文提到的电商场景无法使用 2PC，因为 shopping-service 通过 RPC 接口或者 Rest 接口调用 repo-service 和 order-service 间接访问 repo_db 和 order_db。除非 shopping-service 直接配置 repo_db 和 order_db 作为自己的数据库。

### 2.4. TCC 方案

描述 TCC 方案使用的电商微服务模型如下图所示，在这个模型中，shopping-service 是事务协调者，repo-service 和 order-service 是事务参与者。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibkicg4IQVDS8wvNj4ibVWRuvibTtLy1l2Ry17iao1nHviaRIjFjPgb6I94SQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上文提到，2PC 要求参与者实现了 XA 协议，通常用来解决多个数据库之间的事务问题，比较局限。在多个系统服务利用 api 接口相互调用的时候，就不遵守 XA 协议了，这时候 2PC 就不适用了。现代企业多采用分布式的微服务，因此更多的是要解决多个微服务之间的分布式事务问题。

TCC 就是一种解决多个微服务之间的分布式事务问题的方案。TCC 是 Try、Confirm、Cancel 三个词的缩写，其本质是一个应用层面上的 2PC，同样分为两个阶段：

1）阶段一：准备阶段。协调者调用所有的每个微服务提供的 try 接口，将整个全局事务涉及到的资源锁定住，若锁定成功 try 接口向协调者返回 yes。

2）阶段二：提交阶段。若所有的服务的 try 接口在阶段一都返回 yes，则进入提交阶段，协调者调用所有服务的 confirm 接口，各个服务进行事务提交。如果有任何一个服务的 try 接口在阶段一返回 no 或者超时，则协调者调用所有服务的 cancel 接口。

TCC 的流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibCVeSc9icNq3PDUjqIicRg48fZIMueyGbXicnUcfCI6S1xlXLIGm0GbvTg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里有个关键问题，既然 TCC 是一种服务层面上的 2PC，它是如何解决 2PC 无法应对宕机问题的缺陷的呢？答案是不断重试。由于 try 操作锁住了全局事务涉及的所有资源，保证了业务操作的所有前置条件得到满足，因此无论是 confirm 阶段失败还是 cancel 阶段失败都能通过不断重试直至 confirm 或 cancel 成功（所谓成功就是所有的服务都对 confirm 或者 cancel 返回了 ACK）。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibHXiaXWAwc23nby2vQ1eRUKKcjQETTBrSTkdtobTc2n3IIWwNCqeGKjg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里还有个关键问题，在不断重试 confirm 和 cancel 的过程中（考虑到网络二将军问题的存在）有可能重复进行了 confirm 或 cancel，因此还要再保证 confirm 和 cancel 操作具有幂等性，也就是整个全局事务中，每个参与者只进行一次 confirm 或者 cancel。实现 confirm 和 cancel 操作的幂等性，有很多解决方案，例如每个参与者可以维护一个去重表（可以利用数据库表实现也可以使用内存型 KV 组件实现），记录每个全局事务（以全局事务标记 XID 区分）是否进行过 confirm 或 cancel 操作，若已经进行过，则不再重复执行。

TCC 由支付宝团队提出，被广泛应用于金融系统中。我们用银行账户余额购买基金时，会注意到银行账户中用于购买基金的那部分余额首先会被冻结，由此我们可以猜想，这个过程大概就是 TCC 的第一阶段。

### 2.5. 事务状态表方案

另外有一种类似 TCC 的事务解决方案，借助事务状态表来实现。假设要在一个分布式事务中实现调用 repo-service 扣减库存、调用 order-service 生成订单两个过程。在这种方案中，协调者 shopping-service 维护一张如下的事务状态表：

| 分布式事务 ID   | 事务内容                                                     | 事务状态                                                |
| :-------------- | :----------------------------------------------------------- | :------------------------------------------------------ |
| global_trx_id_1 | 操作 1：调用 repo-service 扣减库存 操作 2：调用 order-service 生成订单 | 状态 1：初始 状态 2：操作 1 成功 状态 3：操作 1、2 成功 |

初始状态为 1，每成功调用一个服务则更新一次状态，最后所有的服务调用成功，状态更新到 3。

有了这张表，就可以启动一个后台任务，扫描这张表中事务的状态，如果一个分布式事务一直（设置一个事务周期阈值）未到状态 3，说明这条事务没有成功执行，于是可以重新调用 repo-service 扣减库存、调用 order-service 生成订单。直至所有的调用成功，事务状态到 3。

如果多次重试仍未使得状态到 3，可以将事务状态置为 error，通过人工介入进行干预。

由于存在服务的调用重试，因此每个服务的接口要根据全局的分布式事务 ID 做幂等，原理同 2.4 节的幂等性实现。

### 2.6. 基于消息中间件的最终一致性事务方案

无论是 2PC & 3PC 还是 TCC、事务状态表，基本都遵守 XA 协议的思想，即这些方案本质上都是事务协调者协调各个事务参与者的本地事务的进度，使所有本地事务共同提交或回滚，最终达成一种全局的 ACID 特性。在协调的过程中，协调者需要收集各个本地事务的当前状态，并根据这些状态发出下一阶段的操作指令。

但是这些全局事务方案由于操作繁琐、时间跨度大，或者在全局事务期间会排他地锁住相关资源，使得整个分布式系统的全局事务的并发度不会太高。这很难满足电商等高并发场景对事务吞吐量的要求，因此互联网服务提供商探索出了很多与 XA 协议背道而驰的分布式事务解决方案。其中利用消息中间件实现的最终一致性全局事务就是一个经典方案。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyib3WvK4Ajlyv4CafVTibExbp0gFZzvictO7CPs5OoUR8nP6YvCNjFIKVEw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

为了表现出这种方案的精髓，我将使用如下的电商系统微服务结构来进行描述：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibibpgOkP52HEkTq9oOIQSTD0ZuicNw7vWkiaGU78o4ZqNJ8CvB2Izw9bxQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在这个模型中，用户不再是请求整合后的 shopping-service 进行下单，而是直接请求 order-service 下单，order-service 一方面添加订单记录，另一方面会调用 repo-service 扣减库存。

这种基于消息中间件的最终一致性事务方案常常被误解成如下的实现方式：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyiblicxUKgiafygpgHzicBzuzM7uUNhqsSa9cEWjibRomrvwpkATlFV3N8ZEw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这种实现方式的流程是：

1）order-service 负责向 MQ server 发送扣减库存消息（repo_deduction_msg）；repo-service 订阅 MQ server 中的扣减库存消息，负责消费消息。

2）用户下单后，order-service 先执行插入订单记录的查询语句，后将 repo_deduction_msg 发到消息中间件中，这两个过程放在一个本地事务中进行，一旦“执行插入订单记录的查询语句”失败，导致事务回滚，“将 repo_deduction_msg 发到消息中间件中”就不会发生；同样，一旦“将 repo_deduction_msg 发到消息中间件中”失败，抛出异常，也会导致“执行插入订单记录的查询语句”操作回滚，最终什么也没有发生。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibSqWLYgN4BX7mlum3mZaUGvZicNwvdIvEr0LTaczRI7TArTIWjukAIVQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

3）repo-service 接收到 repo_deduction_msg 之后，先执行库存扣减查询语句，后向 MQ sever 反馈消息消费完成 ACK，这两个过程放在一个本地事务中进行，一旦“执行库存扣减查询语句”失败，导致事务回滚，“向 MQ sever 反馈消息消费完成 ACK”就不会发生，MQ server 在 Confirm 机制的驱动下会继续向 repo-service 推送该消息，直到整个事务成功提交；同样，一旦“向 MQ sever 反馈消息消费完成 ACK”失败，抛出异常，也对导致“执行库存扣减查询语句”操作回滚，MQ server 在 Confirm 机制的驱动下会继续向 repo-service 推送该消息，直到整个事务成功提交。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibvyqxOPFUxaSh92P9FtvlG4EkCUNyRCOicdtyG6n4ZpBJhXUd4IIpYmw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这种做法看似很可靠。但没有考虑到网络二将军问题的存在，有如下的缺陷：

1）存在网络的 2 将军问题，上面第 2）步中 order-service 发送 repo_deduction_msg 消息失败，对于发送方 order-service 来说，可能是消息中间件没有收到消息；也可能是中间件收到了消息，但向发送方 order-service 响应的 ACK 由于网络故障没有被 order-service 收到。因此 order-service 贸然进行事务回滚，撤销“执行插入订单记录的查询语句”，是不对的，因为 repo-service 那边可能已经接收到 repo_deduction_msg 并成功进行了库存扣减，这样 order-service 和 repo-service 两方就产生了数据不一致问题。

2）repo-service 和 order-service 把网络调用（与 MQ server 通信）放在本地数据库事务里，可能会因为网络延迟产生数据库长事务，影响数据库本地事务的并发度。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibicR1ia8q3vV0bpbkOCYpXYNRTHmlTZU7DuTjpZOjBsZib1BBiaPJEz860g/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)015

以上是被误解的实现方式，下面给出正确的实现方式，如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyib3tVMuahWyAA1Z7s7SfMY81OwoPiavicH2SX8OJfb6J5wCSrEyTbeh6dQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上图所示的方案，利用消息中间件如 rabbitMQ 来实现分布式下单及库存扣减过程的最终一致性。对这幅图做以下说明：

1）order-service 中，

```
在 t_order 表添加订单记录 &&

在 t_local_msg 添加对应的扣减库存消息
```

这两个过程要在一个事务中完成，保证过程的原子性。同样，repo-service 中，

```
检查本次扣库存操作是否已经执行过 &&

执行扣减库存如果本次扣减操作没有执行过 &&

写判重表 &&

向 MQ sever 反馈消息消费完成 ACK
```

这四个过程也要在一个事务中完成，保证过程的原子性。

2）order-service 中有一个后台程序，源源不断地把消息表中的消息传送给消息中间件，成功后则删除消息表中对应的消息。如果失败了，也会不断尝试重传。由于存在网络 2 将军问题，即当 order-service 发送给消息中间件的消息网络超时时，这时候消息中间件可能收到了消息但响应 ACK 失败，也可能没收到，order-service 会再次发送该消息，直至消息中间件响应 ACK 成功，这样可能发生消息的重复发送，不过没关系，只要保证消息不丢失，不乱序就行，后面 repo-service 会做去重处理。

3）消息中间件向 repo-service 推送 repo_deduction_msg，repo-service 成功处理完成后会向中间件响应 ACK，消息中间件收到这个 ACK 才认为 repo-service 成功处理了这条消息，否则会重复推送该消息。但是有这样的情形：repo-service 成功处理了消息，向中间件发送的 ACK 在网络传输中由于网络故障丢失了，导致中间件没有收到 ACK 重新推送了该消息。这也要靠 repo-service 的消息去重特性来避免消息重复消费。

4）在 2）和 3）中提到了两种导致 repo-service 重复收到消息的原因，一是生产者重复生产，二是中间件重传。为了实现业务的幂等性，repo-service 中维护了一张判重表，这张表中记录了被成功处理的消息的 id。repo-service 每次接收到新的消息都先判断消息是否被成功处理过，若是的话不再重复处理。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibV2Pwv7yLCUnRQ8GhSZ1UzPgHx8uiaS0gNnLe2o5icG6h9gGFTiblicm0Yg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

通过这种设计，实现了消息在发送方不丢失，消息在接收方不被重复消费，联合起来就是消息不漏不重，严格实现了 order-service 和 repo-service 的两个数据库中数据的最终一致性。

基于消息中间件的最终一致性全局事务方案是互联网公司在高并发场景中探索出的一种创新型应用模式，利用 MQ 实现微服务之间的异步调用、解耦合和流量削峰，支持全局事务的高并发，并保证分布式数据记录的最终一致性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyib5RqsQaGWCaib0w5w09ZPTrYV2ocrrMEdu9kd15QqwZ2cltTC4vR5O8w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 3. Seata in AT mode 的实现

第 2 章给出了实现实现分布式事务的集中常见的理论模型。本章给出业界开源分布式事务框架 Seata 的实现。

Seata 为用户提供了 AT、TCC、SAGA 和 XA 事务模式。其中 AT 模式是 Seata 主推的事务模式，因此本章分析 Seata in AT mode 的实现。使用 AT 有一个前提，那就是微服务使用的数据库必须是支持事务的关系型数据库。

### 3.1. Seata in AT mode 工作流程概述

Seata 的 AT 模式建立在关系型数据库的本地事务特性的基础之上，通过数据源代理类拦截并解析数据库执行的 SQL，记录自定义的回滚日志，如需回滚，则重放这些自定义的回滚日志即可。AT 模式虽然是根据 XA 事务模型（2PC）演进而来的，但是 AT 打破了 XA 协议的阻塞性制约，在一致性和性能上取得了平衡。

AT 模式是基于 XA 事务模型演进而来的，它的整体机制也是一个改进版本的两阶段提交协议。AT 模式的两个基本阶段是：

1）第一阶段：首先获取本地锁，执行本地事务，业务数据操作和记录回滚日志在同一个本地事务中提交，最后释放本地锁；

2）第二阶段：如需全局提交，异步删除回滚日志即可，这个过程很快就能完成。如需要回滚，则通过第一阶段的回滚日志进行反向补偿。

本章描述 Seata in AT mode 的工作原理使用的电商微服务模型如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibkicg4IQVDS8wvNj4ibVWRuvibTtLy1l2Ry17iao1nHviaRIjFjPgb6I94SQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

在上图中，协调者 shopping-service 先调用参与者 repo-service 扣减库存，后调用参与者 order-service 生成订单。这个业务流使用 Seata in XA mode 后的全局事务流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibV7bX104pLKKVrjlRRHvm7RRF7IwSZHLTzibwQPicOxSOKqEL2wKUOWnw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

上图描述的全局事务执行流程为：

1）shopping-service 向 Seata 注册全局事务，并产生一个全局事务标识 XID

2）将 repo-service.repo_db、order-service.order_db 的本地事务执行到待提交阶段，事务内容包含对 repo-service.repo_db、order-service.order_db 进行的查询操作以及写每个库的 undo_log 记录

3）repo-service.repo_db、order-service.order_db 向 Seata 注册分支事务，并将其纳入该 XID 对应的全局事务范围

4）提交 repo-service.repo_db、order-service.order_db 的本地事务

5）repo-service.repo_db、order-service.order_db 向 Seata 汇报分支事务的提交状态

6）Seata 汇总所有的 DB 的分支事务的提交状态，决定全局事务是该提交还是回滚

7）Seata 通知 repo-service.repo_db、order-service.order_db 提交/回滚本地事务，若需要回滚，采取的是补偿式方法

其中 1）2）3）4）5）属于第一阶段，6）7）属于第二阶段。

### 3.2. Seata in AT mode 工作流程详述

在上面的电商业务场景中，购物服务调用库存服务扣减库存，调用订单服务创建订单，显然这两个调用过程要放在一个事务里面。即：

```
start global_trx

 call 库存服务的扣减库存接口

 call 订单服务的创建订单接口

commit global_trx
```

在库存服务的数据库中，存在如下的库存表 t_repo：

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10001 | 20001           | xx 键盘 | 98    | 200.0 |
| 10002 | 20002           | yy 鼠标 | 199   | 100.0 |

在订单服务的数据库中，存在如下的订单表 t_order：

| id    | order_code    | user_id | production_code | count | price |
| :---- | :------------ | :------ | :-------------- | :---- | :---- |
| 30001 | 2020102500001 | 40001   | 20002           | 1     | 100.0 |
| 30002 | 2020102500001 | 40001   | 20001           | 2     | 400.0 |

现在，id 为 40002 的用户要购买一只商品代码为 20002 的鼠标，整个分布式事务的内容为：

1）在库存服务的库存表中将记录

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10002 | 20002           | yy 鼠标 | 199   | 100.0 |

修改为

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10002 | 20002           | yy 鼠标 | 198   | 100.0 |

2）在订单服务的订单表中添加一条记录

| id    | order_code    | user_id | production_code | count | price |
| :---- | :------------ | :------ | :-------------- | :---- | :---- |
| 30003 | 2020102500002 | 40002   | 20002           | 1     | 100.0 |

以上操作，在 AT 模式的第一阶段的流程图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibpyMee0ZUuQU9QFXyvRnM2W0J972rLkuY7HXGNYVxoNx9bD7ralZ1xg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

从 AT 模式第一阶段的流程来看，分支的本地事务在第一阶段提交完成之后，就会释放掉本地事务锁定的本地记录。这是 AT 模式和 XA 最大的不同点，在 XA 事务的两阶段提交中，被锁定的记录直到第二阶段结束才会被释放。所以 AT 模式减少了锁记录的时间，从而提高了分布式事务的处理效率。AT 模式之所以能够实现第一阶段完成就释放被锁定的记录，是因为 Seata 在每个服务的数据库中维护了一张 undo_log 表，其中记录了对 t_order / t_repo 进行操作前后记录的镜像数据，即便第二阶段发生异常，只需回放每个服务的 undo_log 中的相应记录即可实现全局回滚。

undo_log 的表结构：

| id   | branch_id   | xid         | context | rollback_info                                                | log_status | log_created | log_modified |
| :--- | :---------- | :---------- | :------ | :----------------------------------------------------------- | :--------- | :---------- | :----------- |
| ……   | 分支事务 ID | 全局事务 ID | ……      | 分支事务操作的记录在事务前后的记录镜像，即 beforeImage 和 afterImage | ……         | ……          | ……           |

第一阶段结束之后，Seata 会接收到所有分支事务的提交状态，然后决定是提交全局事务还是回滚全局事务。

1）若所有分支事务本地提交均成功，则 Seata 决定全局提交。Seata 将分支提交的消息发送给各个分支事务，各个分支事务收到分支提交消息后，会将消息放入一个缓冲队列，然后直接向 Seata 返回提交成功。之后，每个本地事务会慢慢处理分支提交消息，处理的方式为：删除相应分支事务的 undo_log 记录。之所以只需删除分支事务的 undo_log 记录，而不需要再做其他提交操作，是因为提交操作已经在第一阶段完成了（这也是 AT 和 XA 不同的地方）。这个过程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibJ2NuHHE4HIxicsfsIVLkqZkTNKRTKhvKGqibv0ZZjcsLEviciahZiaQRghA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

分支事务之所以能够直接返回成功给 Seata，是因为真正关键的提交操作在第一阶段已经完成了，清除 undo_log 日志只是收尾工作，即便清除失败了，也对整个分布式事务不产生实质影响。

2）若任一分支事务本地提交失败，则 Seata 决定全局回滚，将分支事务回滚消息发送给各个分支事务，由于在第一阶段各个服务的数据库上记录了 undo_log 记录，分支事务回滚操作只需根据 undo_log 记录进行补偿即可。全局事务的回滚流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyib2laGp1Qj5tRxEWBmttskicEiad6CxcX4qgN0qswCyWDTuniaUzC9Yc2fw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

这里对图中的 2、3 步做进一步的说明：

1）由于上文给出了 undo_log 的表结构，所以可以通过 xid 和 branch_id 来找到当前分支事务的所有 undo_log 记录；

2）拿到当前分支事务的 undo_log 记录之后，首先要做数据校验，如果 afterImage 中的记录与当前的表记录不一致，说明从第一阶段完成到此刻期间，有别的事务修改了这些记录，这会导致分支事务无法回滚，向 Seata 反馈回滚失败；如果 afterImage 中的记录与当前的表记录一致，说明从第一阶段完成到此刻期间，没有别的事务修改这些记录，分支事务可回滚，进而根据 beforeImage 和 afterImage 计算出补偿 SQL，执行补偿 SQL 进行回滚，然后删除相应 undo_log，向 Seata 反馈回滚成功。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibC4US6rXYBaP2AJuQwwHPpzqrouCdTHmasVtGJksX4up7f5ZGpDPW9Q/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

事务具有 ACID 特性，全局事务解决方案也在尽量实现这四个特性。以上关于 Seata in AT mode 的描述很显然体现出了 AT 的原子性、一致性和持久性。下面着重描述一下 AT 如何保证多个全局事务的隔离性的。

在 AT 中，当多个全局事务操作同一张表时，通过全局锁来保证事务的隔离性。下面描述一下全局锁在读隔离和写隔离两个场景中的作用原理：

1）写隔离（若有全局事务在改/写/删记录，另一个全局事务对同一记录进行的改/写/删要被隔离起来，即写写互斥）：写隔离是为了在多个全局事务对同一张表的同一个字段进行更新操作时，避免一个全局事务在没有被提交成功之前所涉及的数据被其他全局事务修改。写隔离的基本原理是：在第一阶段本地事务（开启本地事务的时候，本地事务会对涉及到的记录加本地锁）提交之前，确保拿到全局锁。如果拿不到全局锁，就不能提交本地事务，并且不断尝试获取全局锁，直至超出重试次数，放弃获取全局锁，回滚本地事务，释放本地事务对记录加的本地锁。

假设有两个全局事务 gtrx_1 和 gtrx_2 在并发操作库存服务，意图扣减如下记录的库存数量：

| id    | production_code | name    | count | price |
| :---- | :-------------- | :------ | :---- | :---- |
| 10002 | 20002           | yy 鼠标 | 198   | 100.0 |

AT 实现写隔离过程的时序图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibLMPDDFCSD7Ruo9MibibuyCTicib54XbMNHibZurfF4Q1zCoL1811AiagpDZw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

图中，1、2、3、4 属于第一阶段，5 属于第二阶段。

在上图中 gtrx_1 和 gtrx_2 均成功提交，如果 gtrx_1 在第二阶段执行回滚操作，那么 gtrx_1 需要重新发起本地事务获取本地锁，然后根据 undo_log 对这个 id=10002 的记录进行补偿式回滚。此时 gtrx_2 仍在等待全局锁，且持有这个 id=10002 的记录的本地锁，因此 gtrx_1 会回滚失败（gtrx_1 回滚需要同时持有全局锁和对 id=10002 的记录加的本地锁），回滚失败的 gtrx_1 会一直重试回滚。直到旁边的 gtrx_2 获取全局锁的尝试次数超过阈值，gtrx_2 会放弃获取全局锁，发起本地回滚，本地回滚结束后，自然会释放掉对这个 id=10002 的记录加的本地锁。此时，gtrx_1 终于可以成功对这个 id=10002 的记录加上了本地锁，同时拿到了本地锁和全局锁的 gtrx_1 就可以成功回滚了。整个过程，全局锁始终在 gtrx_1 手中，并不会发生脏写的问题。整个过程的流程图如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibqdgKCPTq1sc53mfx7KezWibu2qs6CUtNVBia1qmWUVia69ld7HORWKfaA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

2）读隔离（若有全局事务在改/写/删记录，另一个全局事务对同一记录的读取要被隔离起来，即读写互斥）：在数据库本地事务的隔离级别为读已提交、可重复读、串行化时（读未提交不起什么隔离作用，一般不使用），Seata AT 全局事务模型产生的隔离级别是读未提交，也就是说一个全局事务会看到另一个全局事务未全局提交的数据，产生脏读，从前文的第一阶段和第二阶段的流程图中也可以看出这一点。这在最终一致性的分布式事务模型中是可以接受的。

如果要求 AT 模型一定要实现读已提交的事务隔离级别，可以利用 Seata 的 SelectForUpdateExecutor 执行器对 SELECT FOR UPDATE 语句进行代理。SELECT FOR UPDATE 语句在执行时会申请全局锁，如果全局锁已经被其他全局事务占有，则回滚 SELECT FOR UPDATE 语句的执行，释放本地锁，并且重试 SELECT FOR UPDATE 语句。在这个过程中，查询请求会被阻塞，直到拿到全局锁（也就是要读取的记录被其他全局事务提交），读到已被全局事务提交的数据才返回。这个过程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibjnx4IM9rsZERUlOMGP7jH1D1XlvqomjzSWjsAW6icukDEaOFSPN2g3w/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyib9iajtpdPVV9yJJ5cziawYohhNde1nJWE5rhicGvoVJYibEN6gBERvfY1pw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

## 4. 结束语

XA 协议是 X/Open 提出的分布式事务处理标准。文中提到的 2PC、3PC、TCC、本地事务表、Seata in AT mode，无论哪一种，本质都是事务协调者协调各个事务参与者的本地事务的进度，使使所有本地事务共同提交或回滚，最终达成一种全局的 ACID 特性。在协调的过程中，协调者需要收集各个本地事务的当前状态，并根据这些状态发出下一阶段的操作指令。这个思想就是 XA 协议的要义，我们可以说这些事务模型遵守或大致遵守了 XA 协议。

基于消息中间件的最终一致性事务方案是互联网公司在高并发场景中探索出的一种创新型应用模式，利用 MQ 实现微服务之间的异步调用、解耦合和流量削峰，保证分布式数据记录的最终一致性。它显然不遵守 XA 协议。

对于某项技术，可能存在业界标准或协议，但实践者针对具体应用场景的需求或者出于简便的考虑，给出与标准不完全相符的实现，甚至完全不相符的实现，这在工程领域是一种常见的现象。TCC 方案如此、基于消息中间件的最终一致性事务方案如此、Seata in AT mode 模式也如此。而新的标准往往就在这些创新中产生。

。

。

。

。

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyibzUMA505xjOaVyzBicRdPMjnO4L7yvpB7FsVTrF0ofKtwWMibhGPfWCicw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

你难道真的没有发现 2.6 节（基于消息中间件的最终一致性事务方案）给出的正确方案中存在的业务漏洞吗？请各位重新看下这张图，仔细品一品两个微服务的调用方向，把你的想法留在评论区吧 :-)

![图片](https://mmbiz.qpic.cn/mmbiz_png/OyweysCSeLUicibBibJhpkgl32RrSLP1kyib3tVMuahWyAA1Z7s7SfMY81OwoPiavicH2SX8OJfb6J5wCSrEyTbeh6dQ/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)